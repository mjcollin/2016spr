{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying entities in notes\n",
    "\n",
    "Goal of this notebook is to determine how successful entity identification is using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as sql\n",
    "import pyspark.sql.types as types\n",
    "#from pyspark.sql.functions import udf, length\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import pyspark.ml.feature as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232459\n",
      "322668\n"
     ]
    }
   ],
   "source": [
    "# Load Processed Parquet\n",
    "sqlContext = SQLContext(sc)\n",
    "notes = sqlContext.read.parquet(\"../data/idigbio_notes.parquet\")\n",
    "total_records = notes.count()\n",
    "print(total_records)\n",
    "# Small sample of the df\n",
    "notes = notes.sample(withReplacement=False, fraction=0.1)\n",
    "\n",
    "print(notes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322534\n",
      "+--------+\n",
      "|document|\n",
      "+--------+\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|document                                                                                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|♀; On same sheet as DS 136                                                                                                                                                                                                                                   |\n",
      "|“Notes presumably authored by the collector/determiner and accompanying specimen(s) associated with this locality refer to “Moroitanoniha.” The meaning of this term is not clear but may provide additional information concerning the collecting locality.”|\n",
      "|“Notes presumably authored by the collector/determiner and accompanying specimen(s) associated with this locality refer to “Moroitanoniha.” The meaning of this term is not clear but may provide additional information concerning the collecting locality.”|\n",
      "|árbol o arbusto de 1.5--4(6) m de alto, ca 10 cm DAP, abundante; fruto de color verde                                                                                                                                                                        |\n",
      "|árbol de 8 m de alto; fruto rosa; Determined date unspecified.                                                                                                                                                                                               |\n",
      "|árbol de 15 m de alto; flor blanca.                                                                                                                                                                                                                          |\n",
      "|árbol de 10 m de alto; flor blanca                                                                                                                                                                                                                           |\n",
      "|árbol de 10 m de alto                                                                                                                                                                                                                                        |\n",
      "|Árvore. Flor branca. Folha jovens mais escuras. Presença de botões florais. Nome popular: Folha-miúda                                                                                                                                                        |\n",
      "|Árvore.                                                                                                                                                                                                                                                      |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Still have some problems with the document field having nulls and\n",
    "# being empty. Not sure where nulls came from but likely the \n",
    "# empties are really whitespace\n",
    "notes = notes.select(sql.trim(notes[\"document\"]).alias(\"document\"))\\\n",
    "    .dropna(subset=\"document\")\\\n",
    "    .filter(sql.length(\"document\") > 0)\n",
    "notes.cache()  \n",
    "\n",
    "print(notes.count())\n",
    "notes.select(notes[\"document\"])\\\n",
    "    .orderBy(notes[\"document\"])\\\n",
    "    .show(10, truncate=False)\n",
    "notes.select(notes[\"document\"])\\\n",
    "    .orderBy(notes[\"document\"], ascending=False)\\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence detection\n",
    "\n",
    "Does splitting in to sentences matter? Is there enough information to do this with a natural language library or should things like \",\" \"[]\", and \"{}\" be worked in to address semi-structured data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'my', 'name', 'is', 'Mace', 'Windoo']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(s):\n",
    "    '''\n",
    "    Take a string and return a list of tokens split out from it\n",
    "    with the nltk library\n",
    "    '''\n",
    "    return nltk.tokenize.word_tokenize(s)\n",
    "\n",
    "tokens = tokenize(\"Hello, my name is Mace Windoo\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------+\n",
      "|tokens                                                                                 |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "|[cloud, forest, under, sappy, fermenting, bark]                                        |\n",
      "|[Collected, with, 15, ', X, 6, ', seine, &, 30, ', bag, seine, ;, See, field, notes, .]|\n",
      "|[This, specimen, is, stored, in, the, indet, ., Mycena, box]                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_tokenize = sql.udf(tokenize, types.ArrayType(types.StringType()))\n",
    "\n",
    "notes_w_tokens = notes.withColumn('tokens', udf_tokenize(notes['document']))\n",
    "notes_w_tokens.select(notes_w_tokens[\"tokens\"]).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- document: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notes_w_tokens.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9:00-14:00', '.', 'Collected']\n",
      "<type 'list'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "t = [\"9:00-14:00\", \".\", \"Collected\"]\n",
    "pos = nltk.pos_tag(t)\n",
    "print(t)\n",
    "print(type(t))\n",
    "print(type(t[0]))\n",
    "print(type(t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 'NNP', 'word': 'Hello'}, {'tag': ',', 'word': ','}, {'tag': 'PRP$', 'word': 'my'}, {'tag': 'NN', 'word': 'name'}, {'tag': 'VBZ', 'word': 'is'}, {'tag': 'NNP', 'word': 'Mace'}, {'tag': 'NNP', 'word': 'Windoo'}]\n",
      "<type 'list'>\n",
      "<type 'dict'>\n",
      "<type 'str'>\n",
      "{'tag': 'NNP', 'word': 'Hello'}\n"
     ]
    }
   ],
   "source": [
    "def part_of_speech(t):\n",
    "    '''\n",
    "    With a list of tokens, mark their part of speech and return\n",
    "    a list dicts (no native tuple type in dataframes it seems).\n",
    "    '''\n",
    "    pos = nltk.pos_tag(t)\n",
    "    retval = []\n",
    "    for p in pos:\n",
    "        retval.append({\"word\": p[0], \"tag\": p[1]})\n",
    "    return retval\n",
    "\n",
    "pos = part_of_speech(tokens)\n",
    "print(pos)\n",
    "print(type(pos))\n",
    "print(type(pos[0]))\n",
    "print(type(pos[0][\"tag\"]))\n",
    "print(pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pos                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[Map(word -> cloud, tag -> NN), Map(word -> forest, tag -> VBD), Map(word -> under, tag -> IN), Map(word -> sappy, tag -> NN), Map(word -> fermenting, tag -> VBG), Map(word -> bark, tag -> NN)]                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[Map(word -> Collected, tag -> NNP), Map(word -> with, tag -> IN), Map(word -> 15, tag -> CD), Map(word -> ', tag -> POS), Map(word -> X, tag -> NNP), Map(word -> 6, tag -> CD), Map(word -> ', tag -> POS), Map(word -> seine, tag -> NN), Map(word -> &, tag -> CC), Map(word -> 30, tag -> CD), Map(word -> ', tag -> ''), Map(word -> bag, tag -> NN), Map(word -> seine, tag -> NN), Map(word -> ;, tag -> :), Map(word -> See, tag -> NNP), Map(word -> field, tag -> NN), Map(word -> notes, tag -> NNS), Map(word -> ., tag -> .)]|\n",
      "|[Map(word -> This, tag -> DT), Map(word -> specimen, tag -> NN), Map(word -> is, tag -> VBZ), Map(word -> stored, tag -> VBN), Map(word -> in, tag -> IN), Map(word -> the, tag -> DT), Map(word -> indet, tag -> NN), Map(word -> ., tag -> .), Map(word -> Mycena, tag -> NNP), Map(word -> box, tag -> NN)]                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_part_of_speech = sql.udf(part_of_speech, types.ArrayType(\n",
    "                                    types.MapType(\n",
    "                                        types.StringType(),\n",
    "                                        types.StringType()\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "notes_w_tokens2 = notes_w_tokens.withColumn('pos', \n",
    "                                            udf_part_of_speech(notes_w_tokens['tokens']))\n",
    "\n",
    "notes_w_tokens2.select(notes_w_tokens2[\"pos\"]).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- document: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- pos: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notes_w_tokens2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|pos[0][word]|\n",
      "+------------+\n",
      "|cloud       |\n",
      "|Collected   |\n",
      "|This        |\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can we work with maps natively?\n",
    "notes_w_tokens2.select(notes_w_tokens2[\"pos\"][0][\"word\"]).show(3, truncate=False)\n",
    "# YES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'name', 'Mace', 'Windoo']\n"
     ]
    }
   ],
   "source": [
    "# Split out words by type\n",
    "# Can't figure out how to access elements of a map in a filter so \n",
    "# build something that filters the lists for us.\n",
    "def find_pos(pos, part):\n",
    "    '''\n",
    "    Take a list of dicts that represent words tagged with\n",
    "    pos information and return a list of words that match\n",
    "    the requested pos\n",
    "    '''\n",
    "    retval = []\n",
    "    for p in pos:\n",
    "        if p[\"tag\"].startswith(part):\n",
    "            retval.append(p[\"word\"])\n",
    "    return retval\n",
    "\n",
    "print(find_pos(pos, \"NN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can't figure out how to pass a single string to a UDF\n",
    "find_nouns_udf = sql.udf(lambda x: find_pos(x, \"NN\"), types.ArrayType(types.StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| word|\n",
      "+-----+\n",
      "|cloud|\n",
      "|sappy|\n",
      "| bark|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nouns = notes_w_tokens2\\\n",
    "    .select(sql.explode(find_nouns_udf(notes_w_tokens2[\"pos\"])).alias(\"word\"))\n",
    "nouns.cache()\n",
    "nouns.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|         [|43176|\n",
      "|         ]|27293|\n",
      "|      data|16949|\n",
      "|     notes|14227|\n",
      "|      card|11475|\n",
      "|     field|10874|\n",
      "|      trap|10457|\n",
      "|       See|10227|\n",
      "|      soil|10045|\n",
      "|         ||10035|\n",
      "|collection| 9629|\n",
      "|         S| 9596|\n",
      "|    forest| 9478|\n",
      "|         (| 9363|\n",
      "|      Alch| 8557|\n",
      "|    litter| 8281|\n",
      "|       Co.| 7984|\n",
      "|    NOTEBY| 7062|\n",
      "|         m| 7059|\n",
      "|    Number| 6985|\n",
      "|    flight| 6847|\n",
      "| Herbarium| 6837|\n",
      "|Collection| 6719|\n",
      "|         C| 6298|\n",
      "|         )| 6151|\n",
      "|         W| 5951|\n",
      "|  specimen| 5936|\n",
      "|  NOTEDATE| 5689|\n",
      "|   prairie| 5583|\n",
      "|      tall| 5373|\n",
      "+----------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noun_counts = nouns\\\n",
    "    .groupBy(\"word\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"count\", ascending=False)\\\n",
    "    \n",
    "noun_counts.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_counts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                R32W|    1|\n",
      "|                Lehm|    1|\n",
      "|                 CFS|    1|\n",
      "|Gaultheria-Vaccinium|    1|\n",
      "|          Slimbridge|    1|\n",
      "|             CUTLEAF|    1|\n",
      "|              ywllow|    1|\n",
      "|             FURNACE|    1|\n",
      "|             litosol|    1|\n",
      "|            prosrate|    1|\n",
      "|            CAS/HBOI|    1|\n",
      "|                 tim|    1|\n",
      "|          OSAL001066|    1|\n",
      "|              km80.2|    1|\n",
      "|            elkoense|    1|\n",
      "|det_comments:29.8...|    1|\n",
      "|             videtai|    1|\n",
      "|              Pledad|    1|\n",
      "|               IV-19|    1|\n",
      "|                 BRH|    1|\n",
      "|           ATTENDING|    1|\n",
      "|            bivelves|    1|\n",
      "|              Khybeu|    1|\n",
      "|            Oonopsis|    1|\n",
      "|                Exs.|    1|\n",
      "|          VI-27-1933|    1|\n",
      "|               Refer|    1|\n",
      "|          truncicola|    1|\n",
      "|           suberecta|    1|\n",
      "|           rosy-pink|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noun_counts.orderBy(noun_counts[\"count\"]).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word  count\n",
      "0      [  43176\n",
      "1      ]  27293\n",
      "2   data  16949\n",
      "3  notes  14227\n",
      "4   card  11475\n"
     ]
    }
   ],
   "source": [
    "noun_counts_pdf = noun_counts.limit(1000).toPandas()\n",
    "print(noun_counts_pdf.head())\n",
    "\n",
    "tuples = []\n",
    "for l in noun_counts_pdf.iterrows():\n",
    "    tuples.append( (l[1], l[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-d1d9a105dc58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/wordcloud/wordcloud.pyc\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \"\"\"\n\u001b[0;32m    262\u001b[0m         \u001b[1;31m# make sure frequencies are sorted and normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitem1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;31m# largest entry will be 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    712\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m    713\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud().generate_from_frequencies(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And some verbs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
