{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying entities in notes\n",
    "\n",
    "Goal of this notebook is to determine how successful entity identification is using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as sql\n",
    "#from pyspark.sql.functions import udf, length\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import pyspark.ml.feature as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232459\n",
      "3214\n"
     ]
    }
   ],
   "source": [
    "# Load Processed Parquet\n",
    "sqlContext = SQLContext(sc)\n",
    "notes = sqlContext.read.parquet(\"../data/idigbio_notes.parquet\")\n",
    "total_records = notes.count()\n",
    "print(total_records)\n",
    "# Small sample of the df\n",
    "notes = notes.sample(withReplacement=False, fraction=0.001)\n",
    "notes.cache()\n",
    "print(notes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interior label \\\"Polyporus ursinus\\\"  \n",
      "\n",
      " TC 733 (Cekalovic's lot number) \n",
      "\n",
      " Orchard mesophytic. \n",
      "\n",
      "locally common  \n",
      "\n",
      " Cruise 6 \n",
      "\n",
      "  [ #67 | 6 | 30/7/70 ][ Nanorchestes | 1) L | 2) F | 3) N2 | 4) N3 | 5) F | 6) F ] - (data from lid of box) Nanorchestes antarctius | Strandtmann | 4 | coll by : Kay L. Lindsay | 1969-70 | austral su. | Taylor, Wright Valleys | sites #: 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63,64, 65, 66, 67, 68 ]\n",
      "\n",
      "Above crossbars green & black, below bright yellow in preserved animals on arrival.  \n",
      "\n",
      "Location: Slide Box 36, Tray 14, Slide 3; J-31  \n",
      "\n",
      "Designated HOLOTYPE.  \n",
      "\n",
      "  [19°58'18\"S;40°32'05\"W BRASIL: Espirito Santo, Santa Teresa, Est. Biol. Sta. Lúcia, 1-4.VII.1997 840m. W.A. Hoffman, R. Ribeiro, yellow pan traps]\n",
      "\n",
      "Arching perennial herb 1.5 m tall; flowers white; young fruit green.  \n",
      "\n",
      "Expedition by Dr. Barton W. Evermann, Dr. John Van Denburgh and Joseph R. Slevin.  \n",
      "\n",
      "BBP 7.10-C5  \n",
      "\n",
      "Sheet 1 of 2.  \n",
      "\n",
      " malaise trap \n",
      "\n",
      "Abundant. Near top of Seth Bullock Peak. Open woods. Dry, rocky, granite soil. \n",
      "\n",
      "  [01°44'12.8\"S 51°29'56.6\"W, BRASIL: PA, Melgaço, Caxiuanã, Trilha Igarapé Tijucaquara, 21-26.xi.2003, Malaise M18, Na margem do Igapó, AP Aguiar & JDias, P05191]\n",
      "\n",
      "See card for additional data.  \n",
      "\n",
      "All collections of vascular plants and lichen specimens, 2001-2008, by Craig Freeman. Specimens taken in CO, IA, KS, MO, OK, UT, WS, and WY.  Infrequent on limestone outcrops.\n",
      "\n",
      "PLUS CARCASS IN ALCOHOL; SKIN & SKULL NOT FOUND TH  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in notes.head(20):\n",
    "    print(r['document'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notes_pdf = notes.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence detection\n",
    "\n",
    "Does splitting in to sentences matter? Is there enough information to do this with a natural language library or should things like \",\" \"[]\", and \"{}\" be worked in to address semi-structured data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entitys from documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''\n",
    "    Take a string and return a list of tokens split out from it\n",
    "    with the nltk library\n",
    "    '''\n",
    "    if s is not None:\n",
    "        return nltk.tokenize.word_tokenize(s)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "notes_pdf['tokens'] = map(tokenize, notes_pdf['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Interior, label, \\, '', Polyporus, ursinus\\, '']\n",
      "1          [TC, 733, (, Cekalovic, 's, lot, number, )]\n",
      "2                             [Orchard, mesophytic, .]\n",
      "3                                    [locally, common]\n",
      "4                                          [Cruise, 6]\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(notes_pdf.head()['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def part_of_speech(t):\n",
    "    '''\n",
    "    With a list of tokens, mark their part of speech and return\n",
    "    a list of tuples.\n",
    "    '''\n",
    "    return nltk.pos_tag(t)\n",
    "\n",
    "notes_pdf['pos'] = map(part_of_speech, notes_pdf['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [(Interior, NNP), (label, NN), (\\, :), ('', ''...\n",
      "1    [(TC, NNP), (733, CD), ((, CD), (Cekalovic, NN...\n",
      "2           [(Orchard, NNP), (mesophytic, JJ), (., .)]\n",
      "3                        [(locally, RB), (common, JJ)]\n",
      "4                              [(Cruise, NN), (6, CD)]\n",
      "Name: pos, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(notes_pdf.head()['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk(p):\n",
    "    return nltk.chunk.ne_chunk(p)\n",
    "\n",
    "notes_pdf['chunks'] = map(chunk, notes_pdf['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[(Interior, NNP)], (label, NN), (\\, :), ('', ...\n",
      "1    [(TC, NNP), (733, CD), ((, CD), [(Cekalovic, N...\n",
      "2         [[(Orchard, NNP)], (mesophytic, JJ), (., .)]\n",
      "3                        [(locally, RB), (common, JJ)]\n",
      "4                              [(Cruise, NN), (6, CD)]\n",
      "Name: chunks, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(notes_pdf.head()['chunks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with some chunks, can we find any that match ones from darwinCore text? Use word2vec on the \n",
    "Dude, this is a Hard Problem. Need ontology lookup service's code:\n",
    "http://www.ebi.ac.uk/ols/beta/search?q=puma&groupField=iri&start=0&ontology=envo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/alvations/pywsd\n",
    "# This uses it's own term definitions\n",
    "from pywsd.similarity import max_similarity\n",
    "s = \"\"\"locality The specific description of the place. Less specific geographic information can be \n",
    "provided in other geographic terms (higherGeography, continent, country, stateProvince, county, \n",
    "                                    municipality, waterBody, island, islandGroup). This term may \n",
    "contain information modified from the original to correct perceived errors or standardize the description.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('township.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(max_similarity(s, 'town', 'lin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making triples\n",
    "Piece together subject-verb-predicate sets and take a look at the manually even if we don't know what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
