{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as sql\n",
    "#from pyspark.sql.functions import udf, length\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import pyspark.ml.feature as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232459\n"
     ]
    }
   ],
   "source": [
    "# Load Processed Parquet\n",
    "sqlContext = SQLContext(sc)\n",
    "notes = sqlContext.read.parquet(\"../data/idigbio_notes.parquet\")\n",
    "#idbdf = sqlContext.read.parquet(\"../data/idigbio-100k/occurrence.txt.parquet\")\n",
    "total_records = notes.count()\n",
    "print(total_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322534\n"
     ]
    }
   ],
   "source": [
    "# Small sample of the df\n",
    "notes_sm = notes.sample(withReplacement=False, fraction=0.1)\n",
    "notes_sm.cache()\n",
    "print(notes_sm.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text in MB\n",
      "20065\n"
     ]
    }
   ],
   "source": [
    "# How much text is this?\n",
    "print(\"Total text in MB\")\n",
    "print(notes_sm.select(sql.sum(notes_sm['document_len'])).collect()[0][0] / 1024^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard phases:\n",
    "1. tokenize\n",
    "1. remove stop words\n",
    "1. stem\n",
    "1. frequency hist\n",
    "1. wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(uuid=u'e4b5a6e3-c95c-4504-a7b7-f550834278f0', occurrenceID=u'urn:lsid:biosci.ohio-state.edu:osuc_occurrences:OSUC__296595', catalogNumber=u'OSUC 296595', county=u'', institutionCode=u'C.A. Triplehorn Insect Collection, Ohio State University, Columbus, OH (OSUC)', country=u'Mexico', countryCode=u'', stateProvince=u'Distrito Federal', family=u'', recordedBy=u'Dampf, A. (Alfonso)', order=u'', specificEpithet=u'', genus=u'', sex=u'undetermined', scientificName=u'Empoasca', year=u'', month=u'', fieldNotes=u'[Mexico X-26-32 City] [A. Dampf collr]', occurrenceRemarks=u'', eventRemarks=u'', document=u'  [Mexico X-26-32 City] [A. Dampf collr]', document_len=40, fieldNotes_len=38, eventRemarks_len=0, occurrenceRemarks_len=0, tokens=[u'', u'', u'[mexico', u'x-26-32', u'city]', u'[a.', u'dampf', u'collr]'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This creates a new column with a list of tokens in it, not a column of tokens\n",
    "tokenizer = feature.Tokenizer()\n",
    "#print(tokenizer.params)\n",
    "tokenizer.setParams(inputCol='document', outputCol='tokens')\n",
    "notes_tokens = tokenizer.transform(notes_sm)\n",
    "notes_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'', u'', u'[mexico', u'x-26-32', u'city]', u'[a.', u'dampf', u'collr]', u'pit', u'trap']\n",
      "[Row(_1={u'token': u''}), Row(_1={u'token': u''}), Row(_1={u'token': u'[mexico'}), Row(_1={u'token': u'x-26-32'}), Row(_1={u'token': u'city]'}), Row(_1={u'token': u'[a.'}), Row(_1={u'token': u'dampf'}), Row(_1={u'token': u'collr]'}), Row(_1={u'token': u'pit'}), Row(_1={u'token': u'trap'})]\n"
     ]
    }
   ],
   "source": [
    "# flatten to list of tokens & convert to dataframe\n",
    "tokens = notes_tokens.flatMap(lambda x: x['tokens'])\n",
    "print(tokens.take(10))\n",
    "tokens = tokens.map(lambda w: Row({'token': w})).toDF()\n",
    "print(tokens.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyspark.ml.feature.StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
