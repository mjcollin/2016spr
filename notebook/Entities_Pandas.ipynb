{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying entities in notes\n",
    "\n",
    "Goal of this notebook is to determine how successful entity identification is using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as sql\n",
    "#from pyspark.sql.functions import udf, length\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import pyspark.ml.feature as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232459\n",
      "3187\n"
     ]
    }
   ],
   "source": [
    "# Load Processed Parquet\n",
    "sqlContext = SQLContext(sc)\n",
    "notes = sqlContext.read.parquet(\"../data/idigbio_notes.parquet\")\n",
    "total_records = notes.count()\n",
    "print(total_records)\n",
    "# Small sample of the df\n",
    "notes = notes.sample(withReplacement=False, fraction=0.001)\n",
    "notes.cache()\n",
    "print(notes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional data on card.  \n",
      "\n",
      "  [S. VIETNAM Binh Thuy Army Post 24.X.1971 H.J. Harlan]\n",
      "\n",
      "Notes and spore print with collection  \n",
      "\n",
      "  Gloridonus (Laterana) ?arma (DeL. [Prescott N.F., VI-20-37.Ar.] [D.J.&J.N. Knull Collrs.] [â™€]\n",
      "\n",
      "Additional data on card  \n",
      "\n",
      " Malaise trap \n",
      "\n",
      "Flowers pink. Infrequent in locality.  \n",
      "\n",
      "SFRP Kisatchie Nat'l Forest, So. Research Station Herbarium.  \n",
      "\n",
      " Rich, woods, undergrowth. \n",
      "\n",
      "Colorado State Flower, Number in Set: 1  \n",
      "\n",
      "  [Pt.Betsie,Mich. V 30-VI 4,1993 B.&C.Dasch  trap]\n",
      "\n",
      "BBP 7.10-6  \n",
      "\n",
      " tropical moist montane berlese forest litter \n",
      "\n",
      "  ACC:1983-VII -29; preparation:EtOH jar; det_comments:121.0mm\n",
      "\n",
      " lowland tropical forest flight intercept trap \n",
      "\n",
      "BBP 5.12-6  \n",
      "\n",
      " See field notes. \n",
      "\n",
      "Herbaceous plant about 30 cm tall flower stalk arising from basal rosette of leaves, flower white.  \n",
      "\n",
      "Source: MRS.   Captive for several days. parasites  \n",
      "\n",
      "=Lampanyctus. CAS #9037-9156 in one bottle. bottom depth 2012m; see Thompson & Van Cleve 1936, Rept. Intl. Fish. Comm. No.9, pp.88-89 for explanation of hauling procedures. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for r in notes.head(20):\n",
    "    print(r['document'] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notes_pdf = notes.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence detection\n",
    "\n",
    "Does splitting in to sentences matter? Is there enough information to do this with a natural language library or should things like \",\" \"[]\", and \"{}\" be worked in to address semi-structured data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entitys from documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''\n",
    "    Take a string and return a list of tokens split out from it\n",
    "    with the nltk library\n",
    "    '''\n",
    "    if s is not None:\n",
    "        return nltk.tokenize.word_tokenize(s)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "notes_pdf['tokens'] = map(tokenize, notes_pdf['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      [Additional, data, on, card, .]\n",
      "1    [[, S., VIETNAM, Binh, Thuy, Army, Post, 24.X....\n",
      "2         [Notes, and, spore, print, with, collection]\n",
      "3    [Gloridonus, (, Laterana, ), ?, arma, (, DeL, ...\n",
      "4                         [Additional, data, on, card]\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(notes_pdf.head()['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def part_of_speech(t):\n",
    "    '''\n",
    "    With a list of tokens, mark their part of speech and return\n",
    "    a list of tuples.\n",
    "    '''\n",
    "    return nltk.pos_tag(t)\n",
    "\n",
    "notes_pdf['pos'] = map(part_of_speech, notes_pdf['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [(Additional, JJ), (data, NNS), (on, IN), (car...\n",
      "1    [([, NN), (S., NNP), (VIETNAM, NNP), (Binh, NN...\n",
      "2    [(Notes, NNS), (and, CC), (spore, VB), (print,...\n",
      "3    [(Gloridonus, NNP), ((, :), (Laterana, NNP), (...\n",
      "4    [(Additional, JJ), (data, NNS), (on, IN), (car...\n",
      "Name: pos, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(notes_pdf.head()['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk(p):\n",
    "    return nltk.chunk.ne_chunk(p)\n",
    "\n",
    "notes_pdf['chunks'] = map(chunk, notes_pdf['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [(Additional, JJ), (data, NNS), (on, IN), (car...\n",
      "1    [([, NN), (S., NNP), (VIETNAM, NNP), (Binh, NN...\n",
      "2    [(Notes, NNS), (and, CC), (spore, VB), (print,...\n",
      "3    [[(Gloridonus, NNP)], ((, :), [(Laterana, NNP)...\n",
      "4    [(Additional, JJ), (data, NNS), (on, IN), (car...\n",
      "Name: chunks, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(notes_pdf.head()['chunks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with some chunks, can we find any that match ones from darwinCore text? Use word2vec on the \n",
    "Dude, this is a Hard Problem. Need ontology lookup service's code:\n",
    "http://www.ebi.ac.uk/ols/beta/search?q=puma&groupField=iri&start=0&ontology=envo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://github.com/alvations/pywsd\n",
    "# This uses it's own term definitions\n",
    "from pywsd.similarity import max_similarity\n",
    "s = \"\"\"locality The specific description of the place. Less specific geographic information can be \n",
    "provided in other geographic terms (higherGeography, continent, country, stateProvince, county, \n",
    "                                    municipality, waterBody, island, islandGroup). This term may \n",
    "contain information modified from the original to correct perceived errors or standardize the description.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('township.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(max_similarity(s, 'town', 'lin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making triples\n",
    "Piece together subject-verb-predicate sets and take a look at the manually even if we don't know what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S Additional/JJ data/NNS on/IN card/NN ./.)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-faeee641ba64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind_triples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnotes_pdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'chunks'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-faeee641ba64>\u001b[0m in \u001b[0;36mfind_triples\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtriples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"NN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"subject\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "def find_triples(s):\n",
    "    '''\n",
    "    Find s-v-p triples in taged list of tokens, returns\n",
    "    list of dicts with the found triples.\n",
    "    '''\n",
    "    triples = []\n",
    "    t = {}\n",
    "    for (token, tag) in s:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            t[\"subject\"] = token\n",
    "            \n",
    "        #else:\n",
    "        #    triples.append(t)\n",
    "        #    t = {}\n",
    "        \n",
    "    return triples\n",
    "\n",
    "for s in notes_pdf.head(1)['chunks']:\n",
    "    print(s)\n",
    "    \n",
    "print(find_triples(notes_pdf.head(1)['chunks']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
