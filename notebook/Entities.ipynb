{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying entities in notes\n",
    "\n",
    "Goal of this notebook is to determine how successful entity identification is using\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as sql\n",
    "import pyspark.sql.types as types\n",
    "#from pyspark.sql.functions import udf, length\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "import pyspark.ml.feature as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3232459\n",
      "323529\n"
     ]
    }
   ],
   "source": [
    "# Load Processed Parquet\n",
    "sqlContext = SQLContext(sc)\n",
    "notes = sqlContext.read.parquet(\"../data/idigbio_notes.parquet\")\n",
    "total_records = notes.count()\n",
    "print(total_records)\n",
    "# Small sample of the df\n",
    "notes = notes.sample(withReplacement=False, fraction=0.1)\n",
    "\n",
    "print(notes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323369\n",
      "+--------+\n",
      "|document|\n",
      "+--------+\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "|!       |\n",
      "+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|document                                                                                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|♂ & ♀                                                                                                                                                                                                                                                        |\n",
      "|♂                                                                                                                                                                                                                                                            |\n",
      "|♂                                                                                                                                                                                                                                                            |\n",
      "|♀                                                                                                                                                                                                                                                            |\n",
      "|“Notes presumably authored by the collector/determiner and accompanying specimen(s) associated with this locality refer to “Moroitanoniha.” The meaning of this term is not clear but may provide additional information concerning the collecting locality.”|\n",
      "|árbol de 5 m con flor verde y blanco, frutos                                                                                                                                                                                                                 |\n",
      "|árbol de 3 m flor blanca                                                                                                                                                                                                                                     |\n",
      "|árbol 6 m de alto -flores blancas; abundancia regular.                                                                                                                                                                                                       |\n",
      "|Árvore, casca bastante fendilhada e escura. Folhas coriáceas, discolores, face abaxial com tomentos esbranquiçados Inflorescência panícula com botões passados.                                                                                              |\n",
      "|Árvore, 8m.                                                                                                                                                                                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Still have some problems with the document field having nulls and\n",
    "# being empty. Not sure where nulls came from but likely the \n",
    "# empties are really whitespace\n",
    "notes = notes.select(sql.trim(notes[\"document\"]).alias(\"document\"))\\\n",
    "    .dropna(subset=\"document\")\\\n",
    "    .filter(sql.length(\"document\") > 0)\n",
    "notes.cache()  \n",
    "\n",
    "print(notes.count())\n",
    "notes.select(notes[\"document\"])\\\n",
    "    .orderBy(notes[\"document\"])\\\n",
    "    .show(10, truncate=False)\n",
    "notes.select(notes[\"document\"])\\\n",
    "    .orderBy(notes[\"document\"], ascending=False)\\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence detection\n",
    "\n",
    "Does splitting in to sentences matter? Is there enough information to do this with a natural language library or should things like \",\" \"[]\", and \"{}\" be worked in to address semi-structured data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'my', 'name', 'is', 'Mace', 'Windoo']\n"
     ]
    }
   ],
   "source": [
    "from lib.tokens import Tokens\n",
    "tokens = Tokens()\n",
    "print(tokens.tokenize(\"Hello, my name is Mace Windoo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ | Mexico | X-26-32 | City | ] | [ | A. | Dampf | collr | ]\n",
      "\n",
      "\n",
      "Locality | Conditions-Medium | to | small | intermittent | prarie | stream | ; | Flow | stable | ; | Water | clear | but | heavily | stained | ; | Riffles | not | well | defined | ; | Long | pools | interspersed | with | short | pools | ( | choked | with | leaves | Gradient | low | ; | Bottom | consisting | of | hardpan | overlain | by | silt | some | sand | and | rubble | ; | High | dirt | banks | ; | Narrow | tree | fringe | and | cultivated | fields | surrounding | location | ; | Valley | wide | and | shallow | ; | Water | temp | . | 13 | degrees | C | ; | Air | temp | . | 16 | degrees | C. | ; | preparation | : | ETOH | 70 | % | ; | det_comments:27-50mm\n",
      "\n",
      "\n",
      "WHITE | PINE-HEMLOCK-BEECH-MAPLE-OAK | HEAVY | LITTER | MANY | LARGE | TREES\n",
      "\n",
      "\n",
      "flight | intercept | trap\n",
      "\n",
      "\n",
      "None | .\n",
      "\n",
      "\n",
      "new | Delta | Designs | cabinet | ( | 2012 | No | 28 | W. | F. | Henninger | Collection | ; | Cabinet:56 | ; | Drawer:14 | ; | VerbatimCollectionDate | : | September | 15 | , | 1891 | ; | specimen_state | : | skin | ( | 1 | verbatimLocality | : | ; | Chester | Co. | ; | Pennsylvania | ; | USA | ;\n",
      "\n",
      "\n",
      "[ | BRASIL | : | Amazonas | , | Res | . | Ducke | , | 26km | NE | Manaus | ; | added | 12-JUL-2002 | ] | [ | 16-28.ix.1982 | ; | added | 12-JUL-2002 | ]\n",
      "\n",
      "\n",
      "Old | rock | quarry | , | wet | . | Limestone | soil | . | Abundant | .\n",
      "\n",
      "\n",
      "Common | . | Submerged | in | Sheyenne | River | .\n",
      "\n",
      "\n",
      "Additional | data | on | card | .\n",
      "\n",
      "\n",
      "berlese | rainforest | litter\n",
      "\n",
      "\n",
      "tropical | moist | evergreen/cloud | for | litter | berlese | forest | litter\n",
      "\n",
      "\n",
      "Additional | data | on | card | .\n",
      "\n",
      "\n",
      "NOTEBY | LLAMA | , | NOTEDATE | 17-May-08 | : | LLAMA | taxa | ( | Formicidae | , | Curculionidae | , | Diptera | , | Hymenoptera | , | Hemiptera | , | Myriapoda/Arachnida | , | Staphylinidae | , | other | Coleoptera | ) | extracted | ; | July | 2008 | , | Ecosur | .\n",
      "\n",
      "\n",
      "Rays | yellow | ; | abundant\n",
      "\n",
      "\n",
      "[ | Camp | Livingston | | | 2-16-84 | | | Neacanthocinus | obsoletus | | | M | | | from | Loblolly | pine | | | coll | . | W. | J. | Oakes | ] | [ | A48 | | | Trichouropoda | hirsuta | | | Hoyer | 's | 84 | ]\n",
      "\n",
      "\n",
      "Site=U-8 | .\n",
      "\n",
      "\n",
      "[ | Blodgett | Forest | | | VII-'65 | | | ex | Silphidae | ] | [ | Parasitidae | nymph | ]\n",
      "\n",
      "\n",
      "Station=3 | ; | see | GVF | 0124 | for | more | data\n",
      "\n",
      "\n",
      "Abundant | . | Steep | brushy | hillside | . | Sandstone | rock | soil | .\n",
      "\n",
      "\n",
      "NTD | IN | AED | FIELD | NOTES | ON | FILE\n",
      "\n",
      "\n",
      "[ | COLOMBIA | : | Magdalena | PNN | Tayrona | , | Zaino | 11°20'N | 74°2'W | , | 50m | Malaise | 28.vi-17.vii.2000 | R. | Henriquez | , | leg | . | M.301 | ]\n",
      "\n",
      "\n",
      "0.4 | m | high\n",
      "\n",
      "\n",
      "Uncommon | ; | rays | and | disk | yellow\n",
      "\n",
      "\n",
      "NOTEBY | LLAMA | , | NOTEDATE | 19-Jun-10 | : | a | little | bit | of | rain | started | to | fall | . | samples | collected | in | day | , | kept | overnight | in | room | with | AC | , | transported | to | Zamorano | in | bus | , | and | hung | in | Winkler | extraction | bags | ; | samples | sitting | in | collection | bags | for | apprx | . | 24hrs\n",
      "\n",
      "\n",
      "San | Jose | State | University | Collection | .\n",
      "\n",
      "\n",
      "black | face | feathers | growing | in\n",
      "\n",
      "\n",
      "Tree | ca | . | 15 | m | tall | . | Flowers | green | . | Unknown | ID | as | regia | , | but | PF | thinks | it | may | be | sigillata | Dode\n",
      "\n",
      "\n",
      "Paratype | of | Kassina | argyreivittis | ruandae | Laurent | , | 1956 | .\n",
      "\n",
      "\n",
      "Additional | information | on | card\n",
      "\n",
      "\n",
      "On | Sambucus | callicarpa | . | WTU | 60. | ; | Substrate=host | plant | ; | Host=Sambucus | callicarpa | ; | HostFamily=Caprifoliaceae | ; | AdditionalNotesPresent=N | .\n",
      "\n",
      "\n",
      "Common | . | Upland | , | hilly | prairie | . | Dry | , | sandy | , | gravel | soil | .\n",
      "\n",
      "\n",
      "Frequent\n",
      "\n",
      "\n",
      "On | Acarospora | socialis | .\n",
      "\n",
      "\n",
      "5:45-6:00 | am\n",
      "\n",
      "\n",
      "ACC:1982- | XII-10 | ; | preparation:70 | % | ETOH | ; | det_comments:50mm\n",
      "\n",
      "\n",
      "Soft | rush | . | Big | Prairie | Twp | . | 36th | St. | , | S | side | , | 0.5 | mi | E | of | Cottonwood | Ave | . | In | moist | , | disturbed | soil | in | an | open | area | at | the | border | of | wet | deciduous | woods | .\n",
      "\n",
      "\n",
      "Favolus | hexagonalis\n",
      "\n",
      "\n",
      "Low1 | Field | Emergence | Date | June | 28 | 1965 | 577\n",
      "\n",
      "\n",
      "San | Jose | State | University | Collection | . | Born | to | CAS | 191865 | , | 18 | Aug | 1987 | .\n",
      "\n",
      "\n",
      "'Scripps | ' | locality | 60-391A | , | C.N | . | 220 | ; | see | GVF | 2258 | ( | sta | . | 60-241 | ) | for | more | data | .\n",
      "\n",
      "\n",
      "LENGTH | 6.12 | ; | EXTENT | 10.50 | .\n",
      "\n",
      "\n",
      "Few | individuals | observed | near | top | of | ridge | . | Shrub | 0.5 | m | high | . | From | jct | with | Hwy | 67 | in | Wetmore | : | 2.1 | mi | S | on | Hwy | 96 | , | then | 0.4 | mi | W | upslope | on | Cronk | Gulch | Rd | . | San | Isabel | National | Forest | : | Wet | Mountains | . | Douglas | fir-pine | forest | on | steep | , | more | or | less | N-S-trending | ridges | . | Grassy | understorey | with | mountain | mahogany | and | gambel | oak | . | Red | , | rocky-sandy | soil | .\n",
      "\n",
      "\n",
      "Additional | data | on | card | .\n",
      "\n",
      "\n",
      "No | card | . | Stuntz | 3723 | .\n",
      "\n",
      "\n",
      "ACC:1984- | XII-14 | ; | preparation | : | vial | ; | det_comments:22mm\n",
      "\n",
      "\n",
      "Voucher | for | : | Pollinator | Study | # | 22 | .\n",
      "\n",
      "\n",
      "LAB | BORN | , | AGE | 125 | WEEKS\n",
      "\n",
      "\n",
      "ACC:1983-XI | -18 | ; | preparation | : | EtOH | jar | ; | det_comments:139.3mm\n",
      "\n",
      "\n",
      "NOTES | WITH | SPECIMEN | , | PHOTO | SEPARATE | , | IN | DECIDUOUS | WOODS\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_tokenize = sql.udf(tokens.tokenize, types.ArrayType(types.StringType()))\n",
    "\n",
    "notes_w_tokens = notes.withColumn('tokens', udf_tokenize(notes['document']))\n",
    "for r in notes_w_tokens.select(notes_w_tokens[\"tokens\"]).head(50):\n",
    "    print(\" | \".join(r[\"tokens\"]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- document: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notes_w_tokens.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9:00-14:00', '.', 'Collected']\n",
      "<type 'list'>\n",
      "<type 'str'>\n",
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "t = [\"9:00-14:00\", \".\", \"Collected\"]\n",
    "pos = nltk.pos_tag(t)\n",
    "print(t)\n",
    "print(type(t))\n",
    "print(type(t[0]))\n",
    "print(type(t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tag': 'NNP', 'word': 'Hello'}, {'tag': ',', 'word': ','}, {'tag': 'PRP$', 'word': 'my'}, {'tag': 'NN', 'word': 'name'}, {'tag': 'VBZ', 'word': 'is'}, {'tag': 'NNP', 'word': 'Mace'}, {'tag': 'NNP', 'word': 'Windoo'}]\n",
      "<type 'list'>\n",
      "<type 'dict'>\n",
      "<type 'str'>\n",
      "{'tag': 'NNP', 'word': 'Hello'}\n"
     ]
    }
   ],
   "source": [
    "def part_of_speech(t):\n",
    "    '''\n",
    "    With a list of tokens, mark their part of speech and return\n",
    "    a list dicts (no native tuple type in dataframes it seems).\n",
    "    '''\n",
    "    pos = nltk.pos_tag(t)\n",
    "    retval = []\n",
    "    for p in pos:\n",
    "        retval.append({\"word\": p[0], \"tag\": p[1]})\n",
    "    return retval\n",
    "\n",
    "pos = part_of_speech(tokens)\n",
    "print(pos)\n",
    "print(type(pos))\n",
    "print(type(pos[0]))\n",
    "print(type(pos[0][\"tag\"]))\n",
    "print(pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pos                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[Map(word -> cloud, tag -> NN), Map(word -> forest, tag -> VBD), Map(word -> under, tag -> IN), Map(word -> sappy, tag -> NN), Map(word -> fermenting, tag -> VBG), Map(word -> bark, tag -> NN)]                                                                                                                                                                                                                                                                                                                                          |\n",
      "|[Map(word -> Collected, tag -> NNP), Map(word -> with, tag -> IN), Map(word -> 15, tag -> CD), Map(word -> ', tag -> POS), Map(word -> X, tag -> NNP), Map(word -> 6, tag -> CD), Map(word -> ', tag -> POS), Map(word -> seine, tag -> NN), Map(word -> &, tag -> CC), Map(word -> 30, tag -> CD), Map(word -> ', tag -> ''), Map(word -> bag, tag -> NN), Map(word -> seine, tag -> NN), Map(word -> ;, tag -> :), Map(word -> See, tag -> NNP), Map(word -> field, tag -> NN), Map(word -> notes, tag -> NNS), Map(word -> ., tag -> .)]|\n",
      "|[Map(word -> This, tag -> DT), Map(word -> specimen, tag -> NN), Map(word -> is, tag -> VBZ), Map(word -> stored, tag -> VBN), Map(word -> in, tag -> IN), Map(word -> the, tag -> DT), Map(word -> indet, tag -> NN), Map(word -> ., tag -> .), Map(word -> Mycena, tag -> NNP), Map(word -> box, tag -> NN)]                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf_part_of_speech = sql.udf(part_of_speech, types.ArrayType(\n",
    "                                    types.MapType(\n",
    "                                        types.StringType(),\n",
    "                                        types.StringType()\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "notes_w_tokens2 = notes_w_tokens.withColumn('pos', \n",
    "                                            udf_part_of_speech(notes_w_tokens['tokens']))\n",
    "\n",
    "notes_w_tokens2.select(notes_w_tokens2[\"pos\"]).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- document: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- pos: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notes_w_tokens2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|pos[0][word]|\n",
      "+------------+\n",
      "|cloud       |\n",
      "|Collected   |\n",
      "|This        |\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can we work with maps natively?\n",
    "notes_w_tokens2.select(notes_w_tokens2[\"pos\"][0][\"word\"]).show(3, truncate=False)\n",
    "# YES!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'name', 'Mace', 'Windoo']\n"
     ]
    }
   ],
   "source": [
    "# Split out words by type\n",
    "# Can't figure out how to access elements of a map in a filter so \n",
    "# build something that filters the lists for us.\n",
    "def find_pos(pos, part):\n",
    "    '''\n",
    "    Take a list of dicts that represent words tagged with\n",
    "    pos information and return a list of words that match\n",
    "    the requested pos\n",
    "    '''\n",
    "    retval = []\n",
    "    for p in pos:\n",
    "        if p[\"tag\"].startswith(part):\n",
    "            retval.append(p[\"word\"])\n",
    "    return retval\n",
    "\n",
    "print(find_pos(pos, \"NN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can't figure out how to pass a single string to a UDF\n",
    "find_nouns_udf = sql.udf(lambda x: find_pos(x, \"NN\"), types.ArrayType(types.StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| word|\n",
      "+-----+\n",
      "|cloud|\n",
      "|sappy|\n",
      "| bark|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nouns = notes_w_tokens2\\\n",
    "    .select(sql.explode(find_nouns_udf(notes_w_tokens2[\"pos\"])).alias(\"word\"))\n",
    "nouns.cache()\n",
    "nouns.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|         [|43176|\n",
      "|         ]|27293|\n",
      "|      data|16949|\n",
      "|     notes|14227|\n",
      "|      card|11475|\n",
      "|     field|10874|\n",
      "|      trap|10457|\n",
      "|       See|10227|\n",
      "|      soil|10045|\n",
      "|         ||10035|\n",
      "|collection| 9629|\n",
      "|         S| 9596|\n",
      "|    forest| 9478|\n",
      "|         (| 9363|\n",
      "|      Alch| 8557|\n",
      "|    litter| 8281|\n",
      "|       Co.| 7984|\n",
      "|    NOTEBY| 7062|\n",
      "|         m| 7059|\n",
      "|    Number| 6985|\n",
      "|    flight| 6847|\n",
      "| Herbarium| 6837|\n",
      "|Collection| 6719|\n",
      "|         C| 6298|\n",
      "|         )| 6151|\n",
      "|         W| 5951|\n",
      "|  specimen| 5936|\n",
      "|  NOTEDATE| 5689|\n",
      "|   prairie| 5583|\n",
      "|      tall| 5373|\n",
      "+----------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noun_counts = nouns\\\n",
    "    .groupBy(\"word\")\\\n",
    "    .count()\\\n",
    "    .orderBy(\"count\", ascending=False)\\\n",
    "    \n",
    "noun_counts.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_counts.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|                R32W|    1|\n",
      "|                Lehm|    1|\n",
      "|                 CFS|    1|\n",
      "|Gaultheria-Vaccinium|    1|\n",
      "|          Slimbridge|    1|\n",
      "|             CUTLEAF|    1|\n",
      "|              ywllow|    1|\n",
      "|             FURNACE|    1|\n",
      "|             litosol|    1|\n",
      "|            prosrate|    1|\n",
      "|            CAS/HBOI|    1|\n",
      "|                 tim|    1|\n",
      "|          OSAL001066|    1|\n",
      "|              km80.2|    1|\n",
      "|            elkoense|    1|\n",
      "|det_comments:29.8...|    1|\n",
      "|             videtai|    1|\n",
      "|              Pledad|    1|\n",
      "|               IV-19|    1|\n",
      "|                 BRH|    1|\n",
      "|           ATTENDING|    1|\n",
      "|            bivelves|    1|\n",
      "|              Khybeu|    1|\n",
      "|            Oonopsis|    1|\n",
      "|                Exs.|    1|\n",
      "|          VI-27-1933|    1|\n",
      "|               Refer|    1|\n",
      "|          truncicola|    1|\n",
      "|           suberecta|    1|\n",
      "|           rosy-pink|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "noun_counts.orderBy(noun_counts[\"count\"]).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    word  count\n",
      "0      [  43176\n",
      "1      ]  27293\n",
      "2   data  16949\n",
      "3  notes  14227\n",
      "4   card  11475\n"
     ]
    }
   ],
   "source": [
    "noun_counts_pdf = noun_counts.limit(1000).toPandas()\n",
    "print(noun_counts_pdf.head())\n",
    "\n",
    "tuples = []\n",
    "for l in noun_counts_pdf.iterrows():\n",
    "    tuples.append( (l[1], l[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-d1d9a105dc58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/wordcloud/wordcloud.pyc\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \"\"\"\n\u001b[0;32m    262\u001b[0m         \u001b[1;31m# make sure frequencies are sorted and normalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitem1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;31m# largest entry will be 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    712\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m    713\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud().generate_from_frequencies(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And some verbs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
